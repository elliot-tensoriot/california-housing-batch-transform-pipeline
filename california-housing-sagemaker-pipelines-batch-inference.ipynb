{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.51.0 in /opt/conda/lib/python3.7/site-packages (2.88.1)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (0.2.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (1.0.1)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (0.1.5)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (1.5.0)\n",
      "Requirement already satisfied: boto3>=1.20.21 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (1.22.2)\n",
      "Requirement already satisfied: attrs==20.3.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (20.3.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (1.21.6)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (0.2.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (1.0.1)\n",
      "Requirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (3.20.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (20.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.20.21->sagemaker>=2.51.0) (1.0.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.20.21->sagemaker>=2.51.0) (0.5.2)\n",
      "Requirement already satisfied: botocore<1.26.0,>=1.25.2 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.20.21->sagemaker>=2.51.0) (1.25.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=1.4.0->sagemaker>=2.51.0) (2.2.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker>=2.51.0) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker>=2.51.0) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker>=2.51.0) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker>=2.51.0) (2019.3)\n",
      "Requirement already satisfied: dill>=0.3.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.51.0) (0.3.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.51.0) (0.70.12.2)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.51.0) (1.6.6.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.51.0) (0.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.26.0,>=1.25.2->boto3>=1.20.21->sagemaker>=2.51.0) (1.26.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install \"sagemaker>=2.51.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Pipelines Air Quality: Batch Transforms and Model Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates how to train and deploy a model in a SageMaker Pipeline, with both a transformer and an endpoint. It also introduces model monitoring to detect model drift and dataset corruption.\n",
    "\n",
    "The steps in this pipeline include:\n",
    "* Preprocessing the  dataset.\n",
    "* Train a Linear Learner Model.\n",
    "* Creating a Transform Job to run batch inference on the dataset\n",
    "* Creating an endpoint\n",
    "\n",
    "After the pipeline is completed, model monitoring is applied to the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Scenario\n",
    "For the demonstration in this notebook, we examine the relationship between an air pollutant (NO<sub>2</sub>) and weather in a selected city: Dublin, Ireland. \n",
    "\n",
    "The air quality data comes from a long established monitoring station run by the Irish Environmental Protection Agency. The station is located in Rathmines, Dublin, Ireland. Rathmines is an inner suburb of Dublin, about 3 kilometers south from the city center.  Dublin, the capital city of the Republic of Ireland, has a population of approximately one million people. The city is bounded by the sea to the east, mountains to the south, and flat topography to the west and north. The mountains to the south of Dublin affect the wind speed and direction over the city. When the general flow of wind is from the south the mountains deflect the flow to a south-westerly or south-easterly direction.\n",
    "\n",
    "The weather data comes from the long established weather station located at Dublin Airport. Dublin Airport is located on the flat topography to the north of the city. It is about 12 kilometers north of Dublin city center.\n",
    "\n",
    "\n",
    "## The Tools\n",
    "* Amazon SageMaker for machine learning and deploying pipelines. \n",
    "* Amazon Simple Storage Service (Amazon S3) to stage the data for analysis. \n",
    "\n",
    "## The Data\n",
    "Hourly air pollution datasets for the Rathmines monitoring station are published by the Irish Environmental Protection Agency. The data we used spans the years 2011 to 2016. This data is available as Open Data. The provenance of the data is described at the following link, and data can also be downloaded at this link:\n",
    "\n",
    "http://erc.epa.ie/\n",
    "\n",
    "A daily weather data set for Dublin Airport stretching back to 1942 is published by the Irish Meteorological Service (Met. Eireann) on their website under a Creative Commons License.\n",
    "\n",
    "https://www.met.ie/climate/available-data/historical-data\n",
    "\n",
    "For global studies, there is a handy repository of air quality data available on [OpenAQ](https://openaq.org) this data is also available via [Registry of Open Data on AWS](https://registry.opendata.aws/openaq/).\n",
    "\n",
    "\n",
    "## The Method\n",
    "#### Prepaing the data for analysis and loading data from Amazon S3\n",
    "The data is in CSV format. Before being put our Amazon S3 bucket, the data was modified to prepare it for analysis:\n",
    " - Weather Data: The data set contained more information than we needed for the purpose of this proof of concept. To prepare the weather data the following actions with the original dataset were carried out:\n",
    "     - Removed the header, this takes up the first 25 rows of the dataset.\n",
    "     - Converted measurement unit for wind speed from knots to meters per second.\n",
    "     - Selected a subset of the parameters available. Parameters were chosen based on results from scientific papers on this subject.\n",
    "     - The names of the parameters selected were changed to reduce ambiguity.\n",
    "         - ‘rain’ became ‘rain_mm’.  The precipitation amount in mm.\n",
    "         - ‘maxtp’ became ‘maxtemp’. The maximum air temperature in celcius.\n",
    "         - ‘mintp’ became ‘mintemp’. The minimum air temperature in celcius.\n",
    "         -‘cbl’  became ‘pressure_hpa. The mean air pressure in hectopascals.\n",
    "         - ‘wdsp’ became ‘wd_speed_m_per_s’ (and the units converted from knots).\n",
    "         - ‘ddhm’ became ‘winddirection’.\n",
    "         - ‘sun’ became ‘sun_hours’ The sunshine duration.\n",
    "         - ‘evap’ became ‘evap_mm’. Evaporation (mm).\n",
    " - Air Quality Data: Each year of air quality data came in separate files and the units used to measure the pollutants changed from standard units (SI) to an obsolete unit. We decided to only use the years where the SI units are used, this limited us to a time period of 2011 to 2016. These yearly files were merged into one file. \n",
    " - Sample Rate: The weather observations are 24-hour daily averages, and the air quality data came as 1-hour averages. We resampled the air quality data to 24-hour averages and changed the parameter name to indicate this. For example NO<sub>2</sub> became NO2_avg.\n",
    " \n",
    "After this preliminary data transformation, we published the new data in our S3 bucket.\n",
    "\n",
    "### Preparing Amazon SageMaker \n",
    "\n",
    "When opening a SageMaker notebook, we load the relevant libraries into the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "bucket = \"sagemaker-workshop-us-west-1-648739860567\"\n",
    "region = boto3.Session().region_name\n",
    "prefix = \"sagemaker/DEMO-ModelMonitor\"\n",
    "\n",
    "data_capture_prefix = \"{}/datacapture\".format(prefix)\n",
    "s3_capture_upload_path = \"s3://{}/{}\".format(bucket, data_capture_prefix)\n",
    "reports_prefix = \"{}/reports\".format(prefix)\n",
    "s3_report_path = \"s3://{}/{}\".format(bucket, reports_prefix)\n",
    "\n",
    "\n",
    "model_package_group_name = \"Linear-Learner-Air-Quality\"  # Model name in model registry\n",
    "pipeline_name = \"LinearLearnerAirQualityPipeline\"  # SageMaker Pipeline name\n",
    "current_time = time.strftime(\"%m-%d-%H-%M-%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Those libraries will help us analyze the data using pandas, a popular data manipulation tool, as well as numpy, the de-facto scientific library in Python. Seaborn and matplotlib are used to power our visualisations. \n",
    "\n",
    "#### Loading prepared data into Amazon SageMaker from Amazon S3\n",
    "Now that we have the notebook ready for use with the right libraries imported, we can import the data. For this, we will use the pandas library, which is great for exploring and massaging tabular data directly in Python. We can use the **pandas.read_csv** command, supplying it with the location of our data in S3. For both the air pollution and weather data, we changed the column names to something more readable (see the following code, for an air pollution example). We also had to create our own date parser, due to the specific format use for dates in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString, ParameterFloat\n",
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True,expire_after=\"PT8H\")\n",
    "# raw input data\n",
    "nox_data = ParameterString(name=\"NoxData\", default_value='s3://sagemaker-workshop-us-west-1-648739860567/aws-machine-learning-blog/artifacts/air-quality/Dublin_Rathmines_NOx_2011_2016_ugm3_daily.csv')\n",
    "weather_data = ParameterString(name=\"WeatherData\", default_value='s3://sagemaker-workshop-us-west-1-648739860567/aws-machine-learning-blog/artifacts/air-quality/DublinAirportWeatherStationDerived_1942_to_2018.csv')\n",
    "\n",
    "# processing step parameters\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "# training step parameters\n",
    "training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.2xlarge\")\n",
    "training_epochs = ParameterString(name=\"TrainingEpochs\", default_value=\"1\")\n",
    "\n",
    "\n",
    "# Transformer step parameters\n",
    "transformer_instance_type = ParameterString(name=\"TransformInstanceType\", default_value=\"ml.m5.large\")\n",
    "transformer_instance_count = ParameterInteger(name=\"TransformInstanceCount\", default_value=2)\n",
    "max_payload_in_mb = ParameterInteger(name=\"MaxPayloadMB\", default_value=2)\n",
    "output_data_path = ParameterString(name=\"OutputDataS3Path\",default_value=\"s3://{}/air-quality-batch-infer/\".format(bucket))\n",
    "concurrency = ParameterInteger(name=\"MaxConcurrentRequests\",default_value=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Whether exported from data wrangler or already extant, you can use a preprocessing job to clean your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "\n",
    "from pathlib import Path\n",
    "# import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def convert_date_to_right_century(dt):\n",
    "    if dt > datetime.now():\n",
    "        dt -= relativedelta(years=100)\n",
    "    return dt\n",
    "\n",
    "def parse(x):\n",
    "    return datetime.strptime(x, '%d-%b-%y')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    col_names = ['daily_avg', 'nox_avg', 'no_avg', 'no2_avg']\n",
    "    nox_df = pd.read_csv(next(Path('/opt/ml/processing/input/nox').iterdir()),  \n",
    "                        date_parser=parse,\n",
    "                        parse_dates=['Daily_Avg'])\n",
    "    nox_df.columns = col_names\n",
    "    nox_df = nox_df.set_index('daily_avg')\n",
    "    nox_df[\"no2_avg\"] = nox_df[\"no2_avg\"].apply(lambda x: 5 if x <= 0 else x)\n",
    "    \n",
    "    weather_col_names = ['observation_date', 'maxtemp', 'mintemp', 'rain_mm', 'pressure_hpa', 'wd_speed_m_per_s', 'wind_direction', 'sun_hours', 'g_rad', 'evap_mm']\n",
    "    weather_df = pd.read_csv(next(Path('/opt/ml/processing/input/weather').iterdir()), \n",
    "                        date_parser=parse,\n",
    "                        parse_dates=['date'])\n",
    "    weather_df.columns = weather_col_names\n",
    "    weather_df['observation_date'] = weather_df['observation_date'].apply(convert_date_to_right_century)\n",
    "    weather_df = weather_df.set_index('observation_date')\n",
    "    new_weather_df = weather_df['2011-01-01':'2016-12-31']\n",
    "    new_weather_df[['wind_direction']] = new_weather_df[['wind_direction']].apply(pd.to_numeric)\n",
    "    weather_sub_df = new_weather_df[['maxtemp','wd_speed_m_per_s','wind_direction','pressure_hpa','sun_hours']]\n",
    "    no2_df = nox_df[['no2_avg']]\n",
    "    comp_df = pd.merge(weather_sub_df,no2_df, left_index=True, right_index=True)\n",
    "    aq_df = comp_df.iloc[1:].copy()\n",
    "\n",
    "    # Adding wind_speed_direction, the product of wind_speed and the direction\n",
    "    aq_df[\"wind_speed_direction\"] = aq_df.apply(lambda row: row['wd_speed_m_per_s'] * float(row['wind_direction']), axis=1)\n",
    "    aq_train_df = aq_df[aq_df.index.year < 2016]\n",
    "    aq_test_df = aq_df[aq_df.index.year == 2016]\n",
    "    \n",
    "    x_train = aq_train_df.drop('no2_avg',1)\n",
    "    output_path = os.path.join(\"/opt/ml/processing/train\", \"x_train.csv\")\n",
    "    x_train.to_csv(output_path,index=False)#,header=False)\n",
    "    x_test = aq_test_df.drop('no2_avg',1)\n",
    "    output_path = os.path.join(\"/opt/ml/processing/test\", \"x_test.csv\")\n",
    "    x_test.to_csv(output_path,index=False)#,header=False)\n",
    "    aq_train_df.to_csv('/opt/ml/processing/testcsv/x_test_header.csv')\n",
    "    \n",
    "    \n",
    "#     aq_train_df.to_csv(\"/opt/ml/preprocessing/training-dataset-with-header.csv\")\n",
    "#     with open('/op/ml/preprocessing/training-dataset-with-header.csv','rb') as training_data_file:\n",
    "#         baseline_prefix = \"sagemaker/DEMO-ModelMonitor/baselining\"\n",
    "#         bucket = \"sagemaker-workshop-us-west-1-648739860567\"\n",
    "#         s3_key = os.path.join(baseline_prefix, \"data\", \"training-dataset-with-header.csv\")\n",
    "#         boto3.Session().resource(\"s3\").Bucket(bucket).Object(s3_key).upload_fileobj(training_data_file)\n",
    "    \n",
    "    y_train = aq_train_df[[\"no2_avg\"]]\n",
    "    output_path = os.path.join(\"/opt/ml/processing/train\", \"y_train.csv\")\n",
    "    y_train.to_csv(output_path,index=False)#,header=False)\n",
    "    y_test = aq_test_df[[\"no2_avg\"]]\n",
    "    output_path = os.path.join(\"/opt/ml/processing/test\", \"y_test.csv\")\n",
    "    y_test.to_csv(output_path,index=False)#,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  linear-learner-air-quality-processing-j-2022-06-06-16-16-13-009\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': ParameterString(name='NoxData', parameter_type=<ParameterTypeEnum.STRING: 'String'>, default_value='s3://sagemaker-workshop-us-west-1-648739860567/aws-machine-learning-blog/artifacts/air-quality/Dublin_Rathmines_NOx_2011_2016_ugm3_daily.csv'), 'LocalPath': '/opt/ml/processing/input/nox', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'input-2', 'AppManaged': False, 'S3Input': {'S3Uri': ParameterString(name='WeatherData', parameter_type=<ParameterTypeEnum.STRING: 'String'>, default_value='s3://sagemaker-workshop-us-west-1-648739860567/aws-machine-learning-blog/artifacts/air-quality/DublinAirportWeatherStationDerived_1942_to_2018.csv'), 'LocalPath': '/opt/ml/processing/input/weather', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-1-648739860567/linear-learner-air-quality-processing-j-2022-06-06-16-16-13-009/input/code/preprocess.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-workshop-us-west-1-648739860567/workshop/processing/train', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-workshop-us-west-1-648739860567/workshop/processing/test', 'LocalPath': '/opt/ml/processing/test', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'testcsv', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-workshop-us-west-1-648739860567/workshop/processing/testcsv', 'LocalPath': '/opt/ml/processing/testcsv', 'S3UploadMode': 'EndOfJob'}}]\n",
      "....."
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "# Create SKlearn processor object,\n",
    "# The object contains information about what instance type to use, the IAM role to use etc.\n",
    "# A managed processor comes with a preconfigured container, so only specifying version is required.\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    base_job_name=\"linear-learner-air-quality-processing-job\",\n",
    ")\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code=\"preprocess.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(source=nox_data, destination=\"/opt/ml/processing/input/nox\"),\n",
    "        ProcessingInput(source=weather_data, destination=\"/opt/ml/processing/input/weather\")\n",
    "        \n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\",destination=\"s3://sagemaker-workshop-us-west-1-648739860567/workshop/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\",destination=\"s3://sagemaker-workshop-us-west-1-648739860567/workshop/processing/test\"),\n",
    "        ProcessingOutput(output_name=\"testcsv\",source=\"/opt/ml/processing/testcsv\",destination=\"s3://sagemaker-workshop-us-west-1-648739860567/workshop/processing/testcsv\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import time\n",
    "import boto3\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "linear_image = retrieve(\"linear-learner\", boto3.Session().region_name)\n",
    "\n",
    "\n",
    "# Where to store the trained model\n",
    "model_path = f\"s3://{bucket}/{prefix}/model/\"\n",
    "\n",
    "hyperparameters = {\"epochs\": training_epochs}\n",
    "\n",
    "linear_estimator = Estimator(\n",
    "    linear_image,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    volume_size=20,\n",
    "    max_run=3600,\n",
    "    input_mode=\"File\",\n",
    "    output_path=model_path,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "linear_estimator.set_hyperparameters(normalize_data=True,normalize_label=True, predictor_type=\"regressor\", mini_batch_size=32)\n",
    "\n",
    "linear_estimator.fit(\n",
    "    {\"train\": TrainingInput(s3_data=\"s3://sagemaker-workshop-us-west-1-648739860567/workshop/processing/train\",s3_data_type='S3Prefix',content_type=\"text/csv\"),\n",
    "    \"test\": TrainingInput(s3_data=\"s3://sagemaker-workshop-us-west-1-648739860567/workshop/processing/test\",s3_data_type='S3Prefix',content_type=\"text/csv\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_l1 = sagemaker.parameter.ContinuousParameter(1e-7, \n",
    "                                                   1,\n",
    "                                                   scaling_type='Logarithmic')\n",
    "\n",
    "param_wd = sagemaker.parameter.ContinuousParameter(1e-7, \n",
    "                                                   1,\n",
    "                                                   scaling_type='Logarithmic')\n",
    "\n",
    "param_learning_rate = sagemaker.parameter.ContinuousParameter(1e-5,\n",
    "                                                             1,\n",
    "                                                             scaling_type='Logarithmic')\n",
    "\n",
    "hypertuner = sagemaker.tuner.HyperparameterTuner(linear_learner, \n",
    "                             objective_metric_name = 'test:mse', \n",
    "                             hyperparameter_ranges = {\n",
    "                                               'l1' : param_l1,\n",
    "                                               'wd' : param_wd,\n",
    "                                               'learning_rate' : param_learning_rate,\n",
    "                             }, \n",
    "                             metric_definitions=None, \n",
    "                             strategy='Bayesian', \n",
    "                             objective_type='Minimize', \n",
    "                             max_jobs=20, max_parallel_jobs=3,\n",
    "                             early_stopping_type='Off'\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Step \n",
    "\n",
    "The first step in the pipeline will preprocess the data to prepare it for training. The data was already cleaned, as described above, and those steps would be incorporated here when working with the raw data.\n",
    "\n",
    "We create a `SKLearnProcessor` object that has been parameterized, so we can separately track and change the job configuration as needed. As an example, we can increase the instance type size and count to accommodate a growing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "# Create SKlearn processor object,\n",
    "# The object contains information about what instance type to use, the IAM role to use etc.\n",
    "# A managed processor comes with a preconfigured container, so only specifying version is required.\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    role=role,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"linear-learner-air-quality-processing-job\",\n",
    ")\n",
    "\n",
    "# Use the sklearn_processor in a Sagemaker pipelines ProcessingStep\n",
    "step_preprocess_data = ProcessingStep(\n",
    "    name=\"Preprocess-Linear-Learner-Air-Quality-Data\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=nox_data, destination=\"/opt/ml/processing/input/nox\"),\n",
    "        ProcessingInput(source=weather_data, destination=\"/opt/ml/processing/input/weather\")\n",
    "        \n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "        ProcessingOutput(output_name=\"testcsv\",source=\"/opt/ml/processing/testcsv\"),\n",
    "    ],\n",
    "    code=\"preprocess.py\",\n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model step\n",
    "In the second step, the train and validation output from the precious processing step are used to train a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep, TuningStep\n",
    "import time\n",
    "import boto3\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "linear_image = retrieve(\"linear-learner\", boto3.Session().region_name)\n",
    "\n",
    "\n",
    "# Where to store the trained model\n",
    "model_path = f\"s3://{bucket}/{prefix}/model/\"\n",
    "\n",
    "hyperparameters = {\"epochs\": training_epochs}\n",
    "\n",
    "linear_estimator = Estimator(\n",
    "    linear_image,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    volume_size=20,\n",
    "    max_run=3600,\n",
    "    input_mode=\"File\",\n",
    "    output_path=model_path,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "linear_estimator.set_hyperparameters(normalize_data=True,normalize_label=True, predictor_type=\"regressor\", mini_batch_size=32)\n",
    "\n",
    "# param_l1 = sagemaker.parameter.ContinuousParameter(1e-7, \n",
    "#                                                    1,\n",
    "#                                                    scaling_type='Logarithmic')\n",
    "\n",
    "# param_wd = sagemaker.parameter.ContinuousParameter(1e-7, \n",
    "#                                                    1,\n",
    "#                                                    scaling_type='Logarithmic')\n",
    "\n",
    "# param_learning_rate = sagemaker.parameter.ContinuousParameter(1e-5,\n",
    "#                                                              1,\n",
    "#                                                              scaling_type='Logarithmic')\n",
    "\n",
    "# hypertuner = sagemaker.tuner.HyperparameterTuner(linear_estimator, \n",
    "#                              objective_metric_name = 'test:mse', \n",
    "#                              hyperparameter_ranges = {\n",
    "#                                                'l1' : param_l1,\n",
    "#                                                'wd' : param_wd,\n",
    "#                                                'learning_rate' : param_learning_rate,\n",
    "#                              }, \n",
    "#                              metric_definitions=None, \n",
    "#                              strategy='Bayesian', \n",
    "#                              objective_type='Minimize', \n",
    "#                              max_jobs=20, max_parallel_jobs=3,\n",
    "#                              early_stopping_type='Off'\n",
    "#                              )\n",
    "\n",
    "# step_tune_model = TuningStep(\n",
    "#     name=\"Tune-Linear-Learner-Air-Quality-Model\",\n",
    "#     tuner=hypertuner,\n",
    "#     inputs={\n",
    "#         \"train\": TrainingInput(\n",
    "#             s3_data=step_preprocess_data.properties.ProcessingOutputConfig.Outputs[\n",
    "#                 \"train\"\n",
    "#             ].S3Output.S3Uri,\n",
    "#             content_type=\"text/csv\",\n",
    "#         ),\n",
    "#         \"test\": TrainingInput(\n",
    "#             s3_data=step_preprocess_data.properties.ProcessingOutputConfig.Outputs[\n",
    "#                 \"test\"\n",
    "#             ].S3Output.S3Uri,\n",
    "#             content_type=\"text/csv\",\n",
    "#         ),\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# Use the linear_estimator in a Sagemaker pipelines TrainingStep.\n",
    "# NOTE how the input to the training job directly references the output of the previous step.\n",
    "step_train_model = TrainingStep(\n",
    "    name=\"Train-Linear-Learner-Air-Quality-Model\",\n",
    "    estimator=linear_estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_preprocess_data.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data=step_preprocess_data.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "\n",
    "The model is created and the name of the model is provided to the Lambda function for deployment. The `CreateModelStep` dynamically assigns a name to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.step_collections import CreateModelStep\n",
    "from sagemaker.model import Model\n",
    "\n",
    "model = Model(\n",
    "    role=role,\n",
    "    image_uri = linear_image,\n",
    "    model_data=step_train_model.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"Create-Linear-Learner-Air-Quality-Model\",\n",
    "    model=model,\n",
    "    inputs=sagemaker.inputs.CreateModelInput(instance_type=transformer_instance_type),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpoint creation for Model Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting deploy_model_lambda.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile deploy_model_lambda.py\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This Lambda function deploys the model to SageMaker Endpoint. \n",
    "If Endpoint exists, then Endpoint will be updated with new Endpoint Config.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    print(f\"Received Event: {event}\")\n",
    "\n",
    "    current_time = time.strftime(\"%m-%d-%H-%M-%S\", time.localtime())\n",
    "    endpoint_instance_type = event[\"endpoint_instance_type\"]\n",
    "    model_name = event[\"model_name\"]\n",
    "    endpoint_config_name = \"{}-{}\".format(event[\"endpoint_config_name\"], current_time)\n",
    "    endpoint_name = event[\"endpoint_name\"]\n",
    "    s3_capture_upload_path = event[\"s3_capture_upload_path\"]\n",
    "\n",
    "    # Create Endpoint Configuration\n",
    "    create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_config_name,\n",
    "        ProductionVariants=[\n",
    "            {\n",
    "                \"InstanceType\": endpoint_instance_type,\n",
    "                \"InitialVariantWeight\": 1,\n",
    "                \"InitialInstanceCount\": 1,\n",
    "                \"ModelName\": model_name,\n",
    "                \"VariantName\": \"AllTraffic\",\n",
    "            }\n",
    "        ],\n",
    "        DataCaptureConfig= {\n",
    "            'EnableCapture':True,\n",
    "            'InitialSamplingPercentage': 100,\n",
    "            'DestinationS3Uri':s3_capture_upload_path\n",
    "        }\n",
    "    )\n",
    "    print(f\"create_endpoint_config_response: {create_endpoint_config_response}\")\n",
    "\n",
    "    # Check if an endpoint exists. If no - Create new endpoint, if yes - Update existing endpoint\n",
    "    list_endpoints_response = sm_client.list_endpoints(\n",
    "        SortBy=\"CreationTime\",\n",
    "        SortOrder=\"Descending\",\n",
    "        NameContains=endpoint_name,\n",
    "    )\n",
    "    print(f\"list_endpoints_response: {list_endpoints_response}\")\n",
    "\n",
    "    if len(list_endpoints_response[\"Endpoints\"]) > 0:\n",
    "        print(\"Updating Endpoint with new Endpoint Configuration\")\n",
    "        update_endpoint_response = sm_client.update_endpoint(\n",
    "            EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    "        )\n",
    "        print(f\"update_endpoint_response: {update_endpoint_response}\")\n",
    "    else:\n",
    "        print(\"Creating Endpoint\")\n",
    "        create_endpoint_response = sm_client.create_endpoint(\n",
    "            EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    "        )\n",
    "        print(f\"create_endpoint_response: {create_endpoint_response}\")\n",
    "\n",
    "    return {\"statusCode\": 200, \"body\": json.dumps(\"Endpoint Created Successfully\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ARN from existing role: deploy-model-lambda-role\n"
     ]
    }
   ],
   "source": [
    "from iam_helper import create_sagemaker_lambda_role\n",
    "\n",
    "lambda_role = create_sagemaker_lambda_role(\"deploy-model-lambda-role\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.lambda_step import LambdaStep\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "\n",
    "endpoint_config_name = \"linear-learner-air-quality-config\"\n",
    "endpoint_name = \"linear-learner-air-quality-endpoint-\" + current_time\n",
    "\n",
    "deploy_model_lambda_function_name = \"sagemaker-deploy-model-lambda-\" + current_time\n",
    "\n",
    "deploy_model_lambda_function = Lambda(\n",
    "    function_name=deploy_model_lambda_function_name,\n",
    "    execution_role_arn=lambda_role,\n",
    "    script=\"deploy_model_lambda.py\",\n",
    "    handler=\"deploy_model_lambda.lambda_handler\",\n",
    ")\n",
    "\n",
    "step_deploy_predictor = LambdaStep(\n",
    "    name=\"Deploy-Linear-Learner-Air-Quality-Endpoint\",\n",
    "    lambda_func=deploy_model_lambda_function,\n",
    "    inputs={\n",
    "        \"model_name\": step_create_model.properties.ModelName,\n",
    "        \"endpoint_config_name\": endpoint_config_name,\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \"endpoint_instance_type\": transformer_instance_type,\n",
    "        \"model_monitoring_s3_capture_upload_path\": s3_capture_upload_path,\n",
    "    },\n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Transformer Step\n",
    "\n",
    "The model can be either deployed for real time inference or set up to be run on batches of data with a transform job. Creating a `Transformer` from a sagemaker model creates a transformer which can be used to perform batch inference.\n",
    "\n",
    "When creating the transformer, the output defaults to the sagemaker defualt bucket. It can be specified with `output_path` to save to a more desirable location. The other relevant parameters are `instance_count` and `instance_type`, which dictate the number and size of instance that will run the transform job, `max_concurrent_transforms`, which determines how many HTTP requests can be made to each transform container at a time, and `max_payload`, which determines how many megabytes can be sent to a transformer at once (max 4).\n",
    "\n",
    "The transformer can then be passed to the TransformStep, which enables the pipeline to create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_count=transformer_instance_count,\n",
    "    instance_type=transformer_instance_type,\n",
    "    max_concurrent_transforms=concurrency,\n",
    "    max_payload=max_payload_in_mb,\n",
    "    output_path=output_data_path,\n",
    ")\n",
    "\n",
    "step_batch_transform = TransformStep(\n",
    "    name=\"Create-Linear-Learner-Air-Quality-Transformer\",\n",
    "    transformer=transformer,\n",
    "    inputs=\n",
    "        sagemaker.inputs.TransformInput(\n",
    "            data=step_preprocess_data.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri, # Use the same data from S3 as before\n",
    "            data_type='S3Prefix',\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "    \n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Creation: Orchestrate all steps\n",
    "\n",
    "Now that all pipeline steps are created, a pipeline is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create SKlearn processor object,\n",
    "# The object contains information about what instance type to use, the IAM role to use etc.\n",
    "# A managed processor comes with a preconfigured container, so only specifying version is required.\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# Create a Sagemaker Pipeline.\n",
    "# Each parameter for the pipeline must be set as a parameter explicitly when the pipeline is created.\n",
    "# Also pass in each of the steps created above.\n",
    "# Note that the order of execution is determined from each step's dependencies on other steps,\n",
    "# not on the order they are passed in below.\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        training_instance_type,\n",
    "        input_data,\n",
    "        training_epochs,\n",
    "        transformer_instance_type,\n",
    "        transformer_instance_count,\n",
    "        max_payload_in_mb,\n",
    "        output_data_path,\n",
    "        concurrency,\n",
    "        nox_data,\n",
    "        weather_data,\n",
    "    ],\n",
    "    steps=[step_preprocess_data, step_train_model, step_create_model, step_batch_transform, step_deploy_predictor],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the execution steps to check out the status and artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "# definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-west-1:648739860567:pipeline/linearlearnerairqualitypipeline',\n",
       " 'ResponseMetadata': {'RequestId': '2aaf00f9-26e5-4da6-876b-9451499dc9da',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '2aaf00f9-26e5-4da6-876b-9451499dc9da',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '99',\n",
       "   'date': 'Mon, 06 Jun 2022 15:25:35 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute pipeline using the default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for pipeline to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize SageMaker Pipeline\n",
    "In SageMaker Studio, choose `SageMaker Components and registries` in the left pane and under `Pipelines`, click the pipeline that was created. Then all pipeline executions are shown, and the one just created should have a status of `Succeded`. Selecting that execution, the different pipeline steps can be tracked as they execute.\n",
    "\n",
    "![](images/pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a baselining job with training dataset\n",
    "Now that you have the training data ready in Amazon S3, start a job to suggest constraints. DefaultModelMonitor.suggest_baseline(..) starts a ProcessingJob using an Amazon SageMaker provided Model Monitor container to generate the constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_prefix = prefix + \"/baselining\"\n",
    "baseline_data_prefix = baseline_prefix + \"/data\"\n",
    "baseline_results_prefix = baseline_prefix + \"/results\"\n",
    "\n",
    "baseline_data_uri = \"s3://{}/{}\".format(bucket, baseline_data_prefix)\n",
    "baseline_results_uri = \"s3://{}/{}\".format(bucket, baseline_results_prefix)\n",
    "print(\"Baseline data uri: {}\".format(baseline_data_uri))\n",
    "print(\"Baseline results uri: {}\".format(baseline_results_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data_uri + \"/training-dataset-with-header.csv\",\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=json.loads(pipeline.definition())['Steps'][0]['Arguments']['ProcessingOutputConfig']['Outputs'][2]['S3Output']['S3Uri']\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete the pipeline to keep the studio environment tidy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_sagemaker_pipeline(sm_client, pipeline_name):\n",
    "    try:\n",
    "        sm_client.delete_pipeline(\n",
    "            PipelineName=pipeline_name,\n",
    "        )\n",
    "        print(\"{} pipeline deleted\".format(pipeline_name))\n",
    "    except Exception as e:\n",
    "        print(\"{} \\n\".format(e))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_sagemaker_pipeline(client, pipeline_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "\n",
    "### Irish Weather Data:\n",
    "Met Éireann retains Intellectual Property Rights and copyright over our data. If data are published in raw or processed format Met Éireann must be acknowledged as the source. Met Éireann does not accept any liability whatsoever for any error or omission in the data series, their availability, or for any loss or damage arising from their use. This work is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) License.\n",
    "\n",
    "### Irish Air Quality Data:\n",
    "EPA,\"EPA Ireland Archive of Nitrogen Oxides Monitoring Data\". Associated datasets and digitial information objects connected to this resource are available at: Secure Archive For Environmental Research Data (SAFER) managed by Environmental Protection Agency Ireland http://erc.epa.ie/safer/resource?id=216a8992-76e5-102b-aa08-55a7497570d3 (Last Accessed: 2018-06-30) (both require as their data usage license that they be credited)\n",
    "\n",
    "### Wind Rose Code\n",
    "The Air Quality Rose was adapted from Wind Rose code that was published on GitHub under a BSD-license:\n",
    "https://github.com/Geosyntec/cloudside\n",
    "\n",
    "The air quality rose is based on a function called \"rose\" is in the viz.py submodule:\n",
    "https://github.com/Geosyntec/cloudside/blob/master/cloudside/viz.py#L370\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-1:742091327244:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
