{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad5208a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.51.0 in /opt/conda/lib/python3.7/site-packages (2.88.1)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (0.2.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (1.0.1)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (0.1.5)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (1.5.0)\n",
      "Requirement already satisfied: boto3>=1.20.21 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (1.22.2)\n",
      "Requirement already satisfied: attrs==20.3.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (20.3.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (1.21.6)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (0.2.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (1.0.1)\n",
      "Requirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (3.20.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (20.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.20.21->sagemaker>=2.51.0) (1.0.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.20.21->sagemaker>=2.51.0) (0.5.2)\n",
      "Requirement already satisfied: botocore<1.26.0,>=1.25.2 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.20.21->sagemaker>=2.51.0) (1.25.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=1.4.0->sagemaker>=2.51.0) (2.2.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker>=2.51.0) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker>=2.51.0) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker>=2.51.0) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker>=2.51.0) (2019.3)\n",
      "Requirement already satisfied: dill>=0.3.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.51.0) (0.3.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.51.0) (0.70.12.2)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.51.0) (1.6.6.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.51.0) (0.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.26.0,>=1.25.2->boto3>=1.20.21->sagemaker>=2.51.0) (1.26.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install \"sagemaker>=2.51.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330b53a6",
   "metadata": {},
   "source": [
    "# SageMaker Pipelines Air Quality: Batch Transforms and Model Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042e2fc2",
   "metadata": {},
   "source": [
    "This notebook illustrates how to train and deploy a model in a SageMaker Pipeline, with both a transformer and an endpoint. It also introduces model monitoring to detect model drift and dataset corruption.\n",
    "\n",
    "The steps in this pipeline include:\n",
    "* Preprocessing the  dataset.\n",
    "* Train a Linear Learner Model.\n",
    "* Creating a Transform Job to run batch inference on the dataset\n",
    "* Creating an endpoint\n",
    "\n",
    "After the pipeline is completed, model monitoring is applied to the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b96d1a8",
   "metadata": {},
   "source": [
    "## The Scenario\n",
    "For the demonstration in this notebook, we examine the relationship between an air pollutant (NO<sub>2</sub>) and weather in a selected city: Dublin, Ireland. \n",
    "\n",
    "The air quality data comes from a long established monitoring station run by the Irish Environmental Protection Agency. The station is located in Rathmines, Dublin, Ireland. Rathmines is an inner suburb of Dublin, about 3 kilometers south from the city center.  Dublin, the capital city of the Republic of Ireland, has a population of approximately one million people. The city is bounded by the sea to the east, mountains to the south, and flat topography to the west and north. The mountains to the south of Dublin affect the wind speed and direction over the city. When the general flow of wind is from the south the mountains deflect the flow to a south-westerly or south-easterly direction.\n",
    "\n",
    "The weather data comes from the long established weather station located at Dublin Airport. Dublin Airport is located on the flat topography to the north of the city. It is about 12 kilometers north of Dublin city center.\n",
    "\n",
    "\n",
    "## The Tools\n",
    "* Amazon SageMaker for machine learning and deploying pipelines. \n",
    "* Amazon Simple Storage Service (Amazon S3) to stage the data for analysis. \n",
    "\n",
    "## The Data\n",
    "Hourly air pollution datasets for the Rathmines monitoring station are published by the Irish Environmental Protection Agency. The data we used spans the years 2011 to 2016. This data is available as Open Data. The provenance of the data is described at the following link, and data can also be downloaded at this link:\n",
    "\n",
    "http://erc.epa.ie/\n",
    "\n",
    "A daily weather data set for Dublin Airport stretching back to 1942 is published by the Irish Meteorological Service (Met. Eireann) on their website under a Creative Commons License.\n",
    "\n",
    "https://www.met.ie/climate/available-data/historical-data\n",
    "\n",
    "For global studies, there is a handy repository of air quality data available on [OpenAQ](https://openaq.org) this data is also available via [Registry of Open Data on AWS](https://registry.opendata.aws/openaq/).\n",
    "\n",
    "\n",
    "## The Method\n",
    "#### Prepaing the data for analysis and loading data from Amazon S3\n",
    "The data is in CSV format. Before being put our Amazon S3 bucket, the data was modified to prepare it for analysis:\n",
    " - Weather Data: The data set contained more information than we needed for the purpose of this proof of concept. To prepare the weather data the following actions with the original dataset were carried out:\n",
    "     - Removed the header, this takes up the first 25 rows of the dataset.\n",
    "     - Converted measurement unit for wind speed from knots to meters per second.\n",
    "     - Selected a subset of the parameters available. Parameters were chosen based on results from scientific papers on this subject.\n",
    "     - The names of the parameters selected were changed to reduce ambiguity.\n",
    "         - ‘rain’ became ‘rain_mm’.  The precipitation amount in mm.\n",
    "         - ‘maxtp’ became ‘maxtemp’. The maximum air temperature in celcius.\n",
    "         - ‘mintp’ became ‘mintemp’. The minimum air temperature in celcius.\n",
    "         -‘cbl’  became ‘pressure_hpa. The mean air pressure in hectopascals.\n",
    "         - ‘wdsp’ became ‘wd_speed_m_per_s’ (and the units converted from knots).\n",
    "         - ‘ddhm’ became ‘winddirection’.\n",
    "         - ‘sun’ became ‘sun_hours’ The sunshine duration.\n",
    "         - ‘evap’ became ‘evap_mm’. Evaporation (mm).\n",
    " - Air Quality Data: Each year of air quality data came in separate files and the units used to measure the pollutants changed from standard units (SI) to an obsolete unit. We decided to only use the years where the SI units are used, this limited us to a time period of 2011 to 2016. These yearly files were merged into one file. \n",
    " - Sample Rate: The weather observations are 24-hour daily averages, and the air quality data came as 1-hour averages. We resampled the air quality data to 24-hour averages and changed the parameter name to indicate this. For example NO<sub>2</sub> became NO2_avg.\n",
    " \n",
    "After this preliminary data transformation, we published the new data in our S3 bucket.\n",
    "\n",
    "### Preparing Amazon SageMaker \n",
    "\n",
    "When opening a SageMaker notebook, we load the relevant libraries into the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60691d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12123789",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "bucket = \"sagemaker-workshop-us-west-1-648739860567\"\n",
    "region = boto3.Session().region_name\n",
    "prefix = \"sagemaker/DEMO-ModelMonitor\"\n",
    "\n",
    "data_capture_prefix = \"{}/datacapture\".format(prefix)\n",
    "s3_capture_upload_path = \"s3://{}/{}\".format(bucket, data_capture_prefix)\n",
    "reports_prefix = \"{}/reports\".format(prefix)\n",
    "s3_report_path = \"s3://{}/{}\".format(bucket, reports_prefix)\n",
    "\n",
    "\n",
    "model_package_group_name = \"Linear-Learner-Air-Quality\"  # Model name in model registry\n",
    "pipeline_name = \"LinearLearnerAirQualityPipeline\"  # SageMaker Pipeline name\n",
    "current_time = time.strftime(\"%m-%d-%H-%M-%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21267c5",
   "metadata": {},
   "source": [
    "\n",
    "Those libraries will help us analyze the data using pandas, a popular data manipulation tool, as well as numpy, the de-facto scientific library in Python. Seaborn and matplotlib are used to power our visualisations. \n",
    "\n",
    "#### Loading prepared data into Amazon SageMaker from Amazon S3\n",
    "Now that we have the notebook ready for use with the right libraries imported, we can import the data. For this, we will use the pandas library, which is great for exploring and massaging tabular data directly in Python. We can use the **pandas.read_csv** command, supplying it with the location of our data in S3. For both the air pollution and weather data, we changed the column names to something more readable (see the following code, for an air pollution example). We also had to create our own date parser, due to the specific format use for dates in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0e60ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString, ParameterFloat\n",
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True,expire_after=\"PT8H\")\n",
    "# raw input data\n",
    "nox_data = ParameterString(name=\"NoxData\", default_value='s3://sagemaker-workshop-us-west-1-648739860567/aws-machine-learning-blog/artifacts/air-quality/Dublin_Rathmines_NOx_2011_2016_ugm3_daily.csv')\n",
    "weather_data = ParameterString(name=\"WeatherData\", default_value='s3://sagemaker-workshop-us-west-1-648739860567/aws-machine-learning-blog/artifacts/air-quality/DublinAirportWeatherStationDerived_1942_to_2018.csv')\n",
    "\n",
    "# processing step parameters\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "# training step parameters\n",
    "training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.2xlarge\")\n",
    "training_epochs = ParameterString(name=\"TrainingEpochs\", default_value=\"1\")\n",
    "\n",
    "\n",
    "# Transformer step parameters\n",
    "transformer_instance_type = ParameterString(name=\"TransformInstanceType\", default_value=\"ml.m5.large\")\n",
    "transformer_instance_count = ParameterInteger(name=\"TransformInstanceCount\", default_value=2)\n",
    "max_payload_in_mb = ParameterInteger(name=\"MaxPayloadMB\", default_value=2)\n",
    "output_data_path = ParameterString(name=\"OutputDataS3Path\",default_value=\"s3://{}/air-quality-batch-infer/\".format(bucket))\n",
    "concurrency = ParameterInteger(name=\"MaxConcurrentRequests\",default_value=4)\n",
    "\n",
    "endpoint_name = \"aq\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6677c257",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Whether exported from data wrangler or already extant, you can use a preprocessing job to clean your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2af343ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "\n",
    "from pathlib import Path\n",
    "# import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def convert_date_to_right_century(dt):\n",
    "    if dt > datetime.now():\n",
    "        dt -= relativedelta(years=100)\n",
    "    return dt\n",
    "\n",
    "def parse(x):\n",
    "    return datetime.strptime(x, '%d-%b-%y')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    col_names = ['daily_avg', 'nox_avg', 'no_avg', 'no2_avg']\n",
    "    nox_df = pd.read_csv(next(Path('/opt/ml/processing/input/nox').iterdir()),  \n",
    "                        date_parser=parse,\n",
    "                        parse_dates=['Daily_Avg'])\n",
    "    nox_df.columns = col_names\n",
    "    nox_df = nox_df.set_index('daily_avg')\n",
    "    nox_df[\"no2_avg\"] = nox_df[\"no2_avg\"].apply(lambda x: 5 if x <= 0 else x)\n",
    "    \n",
    "    weather_col_names = ['observation_date', 'maxtemp', 'mintemp', 'rain_mm', 'pressure_hpa', 'wd_speed_m_per_s', 'wind_direction', 'sun_hours', 'g_rad', 'evap_mm']\n",
    "    weather_df = pd.read_csv(next(Path('/opt/ml/processing/input/weather').iterdir()), \n",
    "                        date_parser=parse,\n",
    "                        parse_dates=['date'])\n",
    "    weather_df.columns = weather_col_names\n",
    "    weather_df['observation_date'] = weather_df['observation_date'].apply(convert_date_to_right_century)\n",
    "    weather_df = weather_df.set_index('observation_date')\n",
    "    new_weather_df = weather_df['2011-01-01':'2016-12-31']\n",
    "    new_weather_df[['wind_direction']] = new_weather_df[['wind_direction']].apply(pd.to_numeric)\n",
    "    weather_sub_df = new_weather_df[['maxtemp','wd_speed_m_per_s','wind_direction','pressure_hpa','sun_hours']]\n",
    "    no2_df = nox_df[['no2_avg']]\n",
    "    comp_df = pd.merge(weather_sub_df,no2_df, left_index=True, right_index=True)\n",
    "    aq_df = comp_df.iloc[1:].copy()\n",
    "\n",
    "    # Adding wind_speed_direction, the product of wind_speed and the direction\n",
    "    aq_df[\"wind_speed_direction\"] = aq_df.apply(lambda row: row['wd_speed_m_per_s'] * float(row['wind_direction']), axis=1)\n",
    "    aq_train_df = aq_df[aq_df.index.year < 2016]\n",
    "    aq_test_df = aq_df[aq_df.index.year == 2016]\n",
    "    \n",
    "    x_train = aq_train_df.copy()\n",
    "    y_train = x_train.pop('no2_avg')\n",
    "    x_train.insert(0,'labels',y_train.values.astype('float32'))\n",
    "    output_path = os.path.join(\"/opt/ml/processing/train\", \"x_train.csv\")\n",
    "    x_train.to_csv(output_path)#,header=False)\n",
    "    \n",
    "    y_test = aq_test_df.pop('no2_avg')\n",
    "    x_test = x_test.insert(0,'labels',y_test[:,0].values.astype('float32'))\n",
    "    output_path = os.path.join(\"/opt/ml/processing/test\", \"x_test.csv\")\n",
    "    x_test.to_csv(output_path)#,header=False)\n",
    "    x_test_header =aq_train_df.insert(0,'labels',y_train[:,0].values.astype('float32'))\n",
    "    x_test_header.to_csv('/opt/ml/processing/testcsv/x_test_header.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23b2d72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  linear-learner-air-quality-processing-j-2022-06-06-17-28-06-394\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': ParameterString(name='NoxData', parameter_type=<ParameterTypeEnum.STRING: 'String'>, default_value='s3://sagemaker-workshop-us-west-1-648739860567/aws-machine-learning-blog/artifacts/air-quality/Dublin_Rathmines_NOx_2011_2016_ugm3_daily.csv'), 'LocalPath': '/opt/ml/processing/input/nox', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'input-2', 'AppManaged': False, 'S3Input': {'S3Uri': ParameterString(name='WeatherData', parameter_type=<ParameterTypeEnum.STRING: 'String'>, default_value='s3://sagemaker-workshop-us-west-1-648739860567/aws-machine-learning-blog/artifacts/air-quality/DublinAirportWeatherStationDerived_1942_to_2018.csv'), 'LocalPath': '/opt/ml/processing/input/weather', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-1-648739860567/linear-learner-air-quality-processing-j-2022-06-06-17-28-06-394/input/code/preprocess.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-workshop-us-west-1-648739860567/workshop/processing/train', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-workshop-us-west-1-648739860567/workshop/processing/test', 'LocalPath': '/opt/ml/processing/test', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'testcsv', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-workshop-us-west-1-648739860567/workshop/processing/testcsv', 'LocalPath': '/opt/ml/processing/testcsv', 'S3UploadMode': 'EndOfJob'}}]\n",
      ".............................\u001b[34m/miniconda3/lib/python3.7/site-packages/pandas/core/frame.py:3065: SettingWithCopyWarning: \u001b[0m\n",
      "\u001b[34mA value is trying to be set on a copy of a slice from a DataFrame.\u001b[0m\n",
      "\u001b[34mTry using .loc[row_indexer,col_indexer] = value instead\u001b[0m\n",
      "\u001b[34mSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/processing/input/code/preprocess.py\", line 50, in <module>\n",
      "    x_train.to_csv(output_path)#,header=False)\u001b[0m\n",
      "\u001b[34mAttributeError: 'NoneType' object has no attribute 'to_csv'\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Processing job linear-learner-air-quality-processing-j-2022-06-06-17-28-06-394: Failed. Reason: AlgorithmError: See job logs for more information",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-6dab3b3d0a20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mProcessingOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/opt/ml/processing/train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"s3://sagemaker-workshop-us-west-1-648739860567/workshop/processing/train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mProcessingOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/opt/ml/processing/test\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"s3://sagemaker-workshop-us-west-1-648739860567/workshop/processing/test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mProcessingOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"testcsv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/opt/ml/processing/testcsv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"s3://sagemaker-workshop-us-west-1-648739860567/workshop/processing/testcsv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     ],\n\u001b[1;32m     29\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/processing.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, code, inputs, outputs, arguments, wait, logs, job_name, experiment_config, kms_key)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_include_code_in_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/processing.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \"\"\"\n\u001b[1;32m    967\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_processing_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_processing_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_processing_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m   3885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3886\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3887\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ProcessingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3888\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3889\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3337\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3338\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3339\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3340\u001b[0m             )\n\u001b[1;32m   3341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Processing job linear-learner-air-quality-processing-j-2022-06-06-17-28-06-394: Failed. Reason: AlgorithmError: See job logs for more information"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "# Create SKlearn processor object,\n",
    "# The object contains information about what instance type to use, the IAM role to use etc.\n",
    "# A managed processor comes with a preconfigured container, so only specifying version is required.\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    base_job_name=\"linear-learner-air-quality-processing-job\",\n",
    ")\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code=\"preprocess.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(source=nox_data, destination=\"/opt/ml/processing/input/nox\"),\n",
    "        ProcessingInput(source=weather_data, destination=\"/opt/ml/processing/input/weather\")\n",
    "        \n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\",destination=\"s3://sagemaker-workshop-us-west-1-648739860567/workshop/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\",destination=\"s3://sagemaker-workshop-us-west-1-648739860567/workshop/processing/test\"),\n",
    "        ProcessingOutput(output_name=\"testcsv\",source=\"/opt/ml/processing/testcsv\",destination=\"s3://sagemaker-workshop-us-west-1-648739860567/workshop/processing/testcsv\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2548c79",
   "metadata": {},
   "source": [
    "## Training & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b85dca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import time\n",
    "import boto3\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "linear_image = retrieve(\"linear-learner\", boto3.Session().region_name)\n",
    "\n",
    "\n",
    "# Where to store the trained model\n",
    "model_path = f\"s3://{bucket}/{prefix}/model/\"\n",
    "\n",
    "hyperparameters = {\"epochs\": training_epochs}\n",
    "\n",
    "linear_estimator = Estimator(\n",
    "    linear_image,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    volume_size=20,\n",
    "    max_run=3600,\n",
    "    input_mode=\"File\",\n",
    "    output_path=model_path,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "linear_estimator.set_hyperparameters(normalize_data=True,normalize_label=True, predictor_type=\"regressor\", mini_batch_size=32)\n",
    "\n",
    "linear_estimator.fit(\n",
    "    {\"train\": TrainingInput(s3_data=\"s3://sagemaker-workshop-us-west-1-648739860567/workshop/processing/train\",s3_data_type='S3Prefix',content_type=\"text/csv\"),\n",
    "    \"test\": TrainingInput(s3_data=\"s3://sagemaker-workshop-us-west-1-648739860567/workshop/processing/test\",s3_data_type='S3Prefix',content_type=\"text/csv\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f45db7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_l1 = sagemaker.parameter.ContinuousParameter(1e-7, \n",
    "                                                   1,\n",
    "                                                   scaling_type='Logarithmic')\n",
    "\n",
    "param_wd = sagemaker.parameter.ContinuousParameter(1e-7, \n",
    "                                                   1,\n",
    "                                                   scaling_type='Logarithmic')\n",
    "\n",
    "param_learning_rate = sagemaker.parameter.ContinuousParameter(1e-5,\n",
    "                                                             1,\n",
    "                                                             scaling_type='Logarithmic')\n",
    "\n",
    "hypertuner = sagemaker.tuner.HyperparameterTuner(linear_learner, \n",
    "                             objective_metric_name = 'test:mse', \n",
    "                             hyperparameter_ranges = {\n",
    "                                               'l1' : param_l1,\n",
    "                                               'wd' : param_wd,\n",
    "                                               'learning_rate' : param_learning_rate,\n",
    "                             }, \n",
    "                             metric_definitions=None, \n",
    "                             strategy='Bayesian', \n",
    "                             objective_type='Minimize', \n",
    "                             max_jobs=20, max_parallel_jobs=3,\n",
    "                             early_stopping_type='Off'\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d499ec",
   "metadata": {},
   "source": [
    "### Deploying the model with data capture enabled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b815ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_name = \"DEMO-llearner-model-monitor-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"EndpointName={}\".format(endpoint_name))\n",
    "\n",
    "linear_learner_predictor = linear_learner.deploy(initial_instance_count=1,\n",
    "                                 instance_type='ml.t2.medium',\n",
    "                                 endpoint_name=endpoint_name,\n",
    "                                 data_capture_config=data_capture_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b5e506",
   "metadata": {},
   "source": [
    "Once the endpoint is up and running, we can serve predictions by sending data for which we want predictions by calling the predict method. \n",
    "\n",
    "In our case, we have already prepared data for that purpose, which is the the `x_test` variable, containing the features of our data except the target variable, which is the average NO2 concentrations of the day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a870af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = linear_learner_predictor.predict(x_test.values.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23800fe0",
   "metadata": {},
   "source": [
    "The call to predict should be a matter of a few seconds, and the resulting predictions will be saved in the result variable. \n",
    "\n",
    "To be able to get the actual predictions, there’s a slight work of data conversion and transformation to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a85027",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sm_pred = [r.label[\"score\"].float32_tensor.values[0] for r in result]\n",
    "y_sm_test = y_test.values[:, 0].astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec51f6",
   "metadata": {},
   "source": [
    "Let’s get the scores for our metrics based on our actual values and the predicted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1f0f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_sm_test, y_sm_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19d7594",
   "metadata": {},
   "source": [
    "## Step Five: Invoke the deployed model\n",
    "You can now send data to this endpoint to get inferences in real time. Because you enabled the data capture in the previous steps, the request and response payload, along with some additional metadata, is saved in the Amazon Simple Storage Service (Amazon S3) location you have specified in the DataCaptureConfig.\n",
    "\n",
    "This step invokes the endpoint with included sample data for about 3 minutes. Data is captured based on the sampling percentage specified and the capture continues until the data capture option is turned off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dae498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Get a subset of test data for a quick test\n",
    "print(\"Sending test traffic to the endpoint {}. \\nPlease wait...\".format(endpoint_name))\n",
    "\n",
    "for i in range(x_test.shape[0]):\n",
    "    row = x_test.iloc[i].values.astype('float32')\n",
    "#     print(row)\n",
    "    response = llearner_predictor.predict(row)\n",
    "#     print(response)\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaba3e3",
   "metadata": {},
   "source": [
    "### View captured data\n",
    "Now list the data capture files stored in Amazon S3. You should expect to see different files from different time periods organized based on the hour in which the invocation occurred. The format of the Amazon S3 path is:\n",
    "\n",
    "s3://{destination-bucket-prefix}/{endpoint-name}/{variant-name}/yyyy/mm/dd/hh/filename.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6332bf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client(\"s3\")\n",
    "current_endpoint_capture_prefix = \"{}/{}\".format(data_capture_prefix, endpoint_name)\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get(\"Contents\")]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea954a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get(\"Body\").read().decode(\"utf-8\")\n",
    "\n",
    "\n",
    "capture_file = get_obj_body(capture_files[-1])\n",
    "print(capture_file[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15903a3e",
   "metadata": {},
   "source": [
    "### Pretty print of JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f249c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(json.loads(capture_file.split(\"\\n\")[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e2e6fa",
   "metadata": {},
   "source": [
    "As you can see, each inference request is captured in one line in the jsonl file. The line contains both the input and output merged together. In the example, you provided the ContentType as text/csv which is reflected in the observedContentType value. Also, you expose the encoding that you used to encode the input and output payloads in the capture format with the encoding value.\n",
    "\n",
    "To recap, you observed how you can enable capturing the input or output payloads to an endpoint with a new parameter. You have also observed what the captured format looks like in Amazon S3. Next, continue to explore how Amazon SageMaker helps with monitoring the data collected in Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84240397",
   "metadata": {},
   "source": [
    "# Step Six: Model Monitor - Baselining and continuous monitoring\n",
    "\n",
    "In addition to collecting the data, Amazon SageMaker provides the capability for you to monitor and evaluate the data observed by the endpoints. For this:\n",
    "\n",
    "Create a baseline with which you compare the realtime traffic.\n",
    "Once a baseline is ready, setup a schedule to continously evaluate and compare against the baseline.\n",
    "1. Constraint suggestion with baseline/training dataset\n",
    "The training dataset with which you trained the model is usually a good baseline dataset. Note that the training dataset data schema and the inference dataset schema should exactly match (i.e. the number and order of the features).\n",
    "\n",
    "From the training dataset you can ask Amazon SageMaker to suggest a set of baseline constraints and generate descriptive statistics to explore the data. For this example, upload the training dataset that was used to train the pre-trained model included in this example. If you already have it in Amazon S3, you can directly point to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bdcd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy over the training dataset to Amazon S3 (if you already have it in Amazon S3, you could reuse it)\n",
    "baseline_prefix = prefix + \"/baselining\"\n",
    "baseline_data_prefix = baseline_prefix + \"/data\"\n",
    "baseline_results_prefix = baseline_prefix + \"/results\"\n",
    "\n",
    "baseline_data_uri = \"s3://{}/{}\".format(bucket, baseline_data_prefix)\n",
    "baseline_results_uri = \"s3://{}/{}\".format(bucket, baseline_results_prefix)\n",
    "print(\"Baseline data uri: {}\".format(baseline_data_uri))\n",
    "print(\"Baseline results uri: {}\".format(baseline_results_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba5a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_train_df.to_csv(\"./training-dataset-with-header.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdc4610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "training_data_file = open(\"./training-dataset-with-header.csv\", \"rb\")\n",
    "s3_key = os.path.join(baseline_prefix, \"data\", \"training-dataset-with-header.csv\")\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(s3_key).upload_fileobj(training_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1301c7a",
   "metadata": {},
   "source": [
    "### Create a baselining job with training dataset\n",
    "Now that you have the training data ready in Amazon S3, start a job to suggest constraints. DefaultModelMonitor.suggest_baseline(..) starts a ProcessingJob using an Amazon SageMaker provided Model Monitor container to generate the constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf44fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data_uri + \"/training-dataset-with-header.csv\",\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fe5898",
   "metadata": {},
   "source": [
    "## Explore the generated constraints and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bf8ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client(\"s3\")\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=baseline_results_prefix)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get(\"Contents\")]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dd4dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d40ac4",
   "metadata": {},
   "source": [
    "# Analyze collected data for data quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c2474",
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_schedule_name = \"DEMO-llinear-pred-model-monitor-schedule-\" + strftime(\n",
    "    \"%Y-%m-%d-%H-%M-%S\", gmtime()\n",
    ")\n",
    "\n",
    "print(mon_schedule_name)\n",
    "print(s3_report_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "\n",
    "# reports_prefix = \"{}/reports\".format(prefix)\n",
    "# s3_report_path = \"s3://{}/{}\".format(bucket, reports_prefix)\n",
    "\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=llearner_predictor.endpoint,\n",
    "    # record_preprocessor_script=pre_processor_script,\n",
    "    output_s3_uri=s3_report_path,\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3015ca",
   "metadata": {},
   "source": [
    "### Start generating some artificial traffic\n",
    "The cell below starts a thread to send some traffic to the endpoint. Note that you need to stop the kernel to terminate this thread. If there is no traffic, the monitoring jobs are marked as Failed since there is no data to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d8896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.to_csv(\"./test-dataset-input-cols.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f24da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send traffic all at once\n",
    "\n",
    "for i in range(x_test.shape[0]):\n",
    "    row = x_test.iloc[i].values.astype('float32')\n",
    "#     print(row)\n",
    "    response = llearner_predictor.predict(row)\n",
    "#     print(response)\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7596449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send traffic on a background thread continuously.\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "from threading import Thread\n",
    "from time import sleep\n",
    "\n",
    "endpoint_name = llearner_predictor.endpoint\n",
    "runtime_client = sm_session.sagemaker_runtime_client\n",
    "\n",
    "# (just repeating code from above for convenience/ able to run this section independently)\n",
    "def invoke_endpoint(ep_name, file_name, runtime_client):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        for row in f:\n",
    "            payload = row.rstrip(\"\\n\")\n",
    "            response = runtime_client.invoke_endpoint(\n",
    "                EndpointName=ep_name, ContentType=\"text/csv\", Body=payload\n",
    "            )\n",
    "            response[\"Body\"].read()\n",
    "            time.sleep(1)\n",
    "\n",
    "def invoke_api():\n",
    "    for i in range(x_test.shape[0]):\n",
    "        row = x_test.iloc[i].values.astype('float32')\n",
    "    #     print(row)\n",
    "        response = llearner_predictor.predict(row)\n",
    "    #     print(response)\n",
    "        time.sleep(1)\n",
    "\n",
    "def invoke_endpoint_forever():\n",
    "    while True:\n",
    "        try:\n",
    "#             invoke_endpoint(endpoint_name, \"./test-dataset-input-cols.csv\", runtime_client)\n",
    "            invoke_api()\n",
    "        except runtime_client.exceptions.ValidationError:\n",
    "            pass\n",
    "\n",
    "\n",
    "thread = Thread(target=invoke_endpoint_forever)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f07b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_schedule_result = my_default_monitor.describe_schedule()\n",
    "print(\"Schedule status: {}\".format(desc_schedule_result[\"MonitoringScheduleStatus\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb38d357",
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_executions = my_default_monitor.list_executions()\n",
    "print(\n",
    "    \"We created a hourly schedule above that begins executions ON the hour (plus 0-20 min buffer.\\nWe will have to wait till we hit the hour...\"\n",
    ")\n",
    "\n",
    "while len(mon_executions) == 0:\n",
    "    print(\"Waiting for the first execution to happen...\")\n",
    "    time.sleep(60)\n",
    "    mon_executions = my_default_monitor.list_executions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a980ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_execution = mon_executions[-1]  # Latest execution's index is -1, second to last is -2, etc\n",
    "# time.sleep(60)\n",
    "# latest_execution.wait(logs=False)\n",
    "\n",
    "print(\"Latest execution status: {}\".format(latest_execution.describe()[\"ProcessingJobStatus\"]))\n",
    "print(\"Latest execution result: {}\".format(latest_execution.describe()[\"ExitMessage\"]))\n",
    "\n",
    "latest_job = latest_execution.describe()\n",
    "if latest_job[\"ProcessingJobStatus\"] != \"Completed\":\n",
    "    print(\n",
    "        \"====STOP==== \\n No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8036e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "violations = my_default_monitor.latest_monitoring_constraint_violations()\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "constraints_df = pd.io.json.json_normalize(violations.body_dict[\"violations\"])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b53450",
   "metadata": {},
   "source": [
    "# Create Cloud Watch Alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8da9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_client = boto3.Session().client('cloudwatch')\n",
    "\n",
    "alarm_name='BASELINE_DRIFT_FEATURE_'\n",
    "alarm_desc='Trigger an cloudwatch alarm when the feature age drifts away from the baseline'\n",
    "feature_age_drift_threshold=0.1 ##Setting this threshold purposefully slow to see the alarm quickly.\n",
    "metric_name='feature_baseline_drift'\n",
    "namespace='aws/sagemaker/Endpoints/data-metrics'\n",
    "\n",
    "endpoint_name=endpoint_name\n",
    "monitoring_schedule_name=mon_schedule_name\n",
    "\n",
    "cw_client.put_metric_alarm(\n",
    "    AlarmName=alarm_name,\n",
    "    AlarmDescription=alarm_desc,\n",
    "    ActionsEnabled=True,\n",
    "    AlarmActions=[sns_notifications_topic],\n",
    "    MetricName=metric_name,\n",
    "    Namespace=namespace,\n",
    "    Statistic='Average',\n",
    "    Dimensions=[\n",
    "        {\n",
    "            'Name': 'Endpoint',\n",
    "            'Value': endpoint_name\n",
    "        },\n",
    "        {\n",
    "            'Name': 'MonitoringSchedule',\n",
    "            'Value': monitoring_schedule_name\n",
    "        }\n",
    "    ],\n",
    "    Period=600,\n",
    "    EvaluationPeriods=1,\n",
    "    DatapointsToAlarm=1,\n",
    "    Threshold=feature_age_drift_threshold,\n",
    "    ComparisonOperator='GreaterThanOrEqualToThreshold',\n",
    "    TreatMissingData='breaching'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3fe091",
   "metadata": {},
   "source": [
    "## Processing Step \n",
    "\n",
    "The first step in the pipeline will preprocess the data to prepare it for training. The data was already cleaned, as described above, and those steps would be incorporated here when working with the raw data.\n",
    "\n",
    "We create a `SKLearnProcessor` object that has been parameterized, so we can separately track and change the job configuration as needed. As an example, we can increase the instance type size and count to accommodate a growing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1304a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "# Create SKlearn processor object,\n",
    "# The object contains information about what instance type to use, the IAM role to use etc.\n",
    "# A managed processor comes with a preconfigured container, so only specifying version is required.\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    role=role,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"linear-learner-air-quality-processing-job\",\n",
    ")\n",
    "\n",
    "# Use the sklearn_processor in a Sagemaker pipelines ProcessingStep\n",
    "step_preprocess_data = ProcessingStep(\n",
    "    name=\"Preprocess-Linear-Learner-Air-Quality-Data\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=nox_data, destination=\"/opt/ml/processing/input/nox\"),\n",
    "        ProcessingInput(source=weather_data, destination=\"/opt/ml/processing/input/weather\")\n",
    "        \n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "        ProcessingOutput(output_name=\"testcsv\",source=\"/opt/ml/processing/testcsv\"),\n",
    "    ],\n",
    "    code=\"preprocess.py\",\n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d349727c",
   "metadata": {},
   "source": [
    "## Train model step\n",
    "In the second step, the train and validation output from the precious processing step are used to train a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b68a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep, TuningStep\n",
    "import time\n",
    "import boto3\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "linear_image = retrieve(\"linear-learner\", boto3.Session().region_name)\n",
    "\n",
    "\n",
    "# Where to store the trained model\n",
    "model_path = f\"s3://{bucket}/{prefix}/model/\"\n",
    "\n",
    "hyperparameters = {\"epochs\": training_epochs}\n",
    "\n",
    "linear_estimator = Estimator(\n",
    "    linear_image,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    volume_size=20,\n",
    "    max_run=3600,\n",
    "    input_mode=\"File\",\n",
    "    output_path=model_path,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "linear_estimator.set_hyperparameters(normalize_data=True,normalize_label=True, predictor_type=\"regressor\", mini_batch_size=32)\n",
    "\n",
    "# param_l1 = sagemaker.parameter.ContinuousParameter(1e-7, \n",
    "#                                                    1,\n",
    "#                                                    scaling_type='Logarithmic')\n",
    "\n",
    "# param_wd = sagemaker.parameter.ContinuousParameter(1e-7, \n",
    "#                                                    1,\n",
    "#                                                    scaling_type='Logarithmic')\n",
    "\n",
    "# param_learning_rate = sagemaker.parameter.ContinuousParameter(1e-5,\n",
    "#                                                              1,\n",
    "#                                                              scaling_type='Logarithmic')\n",
    "\n",
    "# hypertuner = sagemaker.tuner.HyperparameterTuner(linear_estimator, \n",
    "#                              objective_metric_name = 'test:mse', \n",
    "#                              hyperparameter_ranges = {\n",
    "#                                                'l1' : param_l1,\n",
    "#                                                'wd' : param_wd,\n",
    "#                                                'learning_rate' : param_learning_rate,\n",
    "#                              }, \n",
    "#                              metric_definitions=None, \n",
    "#                              strategy='Bayesian', \n",
    "#                              objective_type='Minimize', \n",
    "#                              max_jobs=20, max_parallel_jobs=3,\n",
    "#                              early_stopping_type='Off'\n",
    "#                              )\n",
    "\n",
    "# step_tune_model = TuningStep(\n",
    "#     name=\"Tune-Linear-Learner-Air-Quality-Model\",\n",
    "#     tuner=hypertuner,\n",
    "#     inputs={\n",
    "#         \"train\": TrainingInput(\n",
    "#             s3_data=step_preprocess_data.properties.ProcessingOutputConfig.Outputs[\n",
    "#                 \"train\"\n",
    "#             ].S3Output.S3Uri,\n",
    "#             content_type=\"text/csv\",\n",
    "#         ),\n",
    "#         \"test\": TrainingInput(\n",
    "#             s3_data=step_preprocess_data.properties.ProcessingOutputConfig.Outputs[\n",
    "#                 \"test\"\n",
    "#             ].S3Output.S3Uri,\n",
    "#             content_type=\"text/csv\",\n",
    "#         ),\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# Use the linear_estimator in a Sagemaker pipelines TrainingStep.\n",
    "# NOTE how the input to the training job directly references the output of the previous step.\n",
    "step_train_model = TrainingStep(\n",
    "    name=\"Train-Linear-Learner-Air-Quality-Model\",\n",
    "    estimator=linear_estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_preprocess_data.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data=step_preprocess_data.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25af037a",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "\n",
    "The model is created and the name of the model is provided to the Lambda function for deployment. The `CreateModelStep` dynamically assigns a name to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "d66c48ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.step_collections import CreateModelStep\n",
    "from sagemaker.model import Model\n",
    "\n",
    "model = Model(\n",
    "    role=role,\n",
    "    image_uri = linear_image,\n",
    "    model_data=step_train_model.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"Create-Linear-Learner-Air-Quality-Model\",\n",
    "    model=model,\n",
    "    inputs=sagemaker.inputs.CreateModelInput(instance_type=transformer_instance_type),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236ac44c",
   "metadata": {},
   "source": [
    "## Endpoint creation for Model Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "6f66ce48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting deploy_model_lambda.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile deploy_model_lambda.py\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This Lambda function deploys the model to SageMaker Endpoint. \n",
    "If Endpoint exists, then Endpoint will be updated with new Endpoint Config.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    print(f\"Received Event: {event}\")\n",
    "\n",
    "    current_time = time.strftime(\"%m-%d-%H-%M-%S\", time.localtime())\n",
    "    endpoint_instance_type = event[\"endpoint_instance_type\"]\n",
    "    model_name = event[\"model_name\"]\n",
    "    endpoint_config_name = \"{}-{}\".format(event[\"endpoint_config_name\"], current_time)\n",
    "    endpoint_name = event[\"endpoint_name\"]\n",
    "    s3_capture_upload_path = event[\"s3_capture_upload_path\"]\n",
    "\n",
    "    # Create Endpoint Configuration\n",
    "    create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_config_name,\n",
    "        ProductionVariants=[\n",
    "            {\n",
    "                \"InstanceType\": endpoint_instance_type,\n",
    "                \"InitialVariantWeight\": 1,\n",
    "                \"InitialInstanceCount\": 1,\n",
    "                \"ModelName\": model_name,\n",
    "                \"VariantName\": \"AllTraffic\",\n",
    "            }\n",
    "        ],\n",
    "        DataCaptureConfig= {\n",
    "            'EnableCapture':True,\n",
    "            'InitialSamplingPercentage': 100,\n",
    "            'DestinationS3Uri':s3_capture_upload_path\n",
    "        }\n",
    "    )\n",
    "    print(f\"create_endpoint_config_response: {create_endpoint_config_response}\")\n",
    "\n",
    "    # Check if an endpoint exists. If no - Create new endpoint, if yes - Update existing endpoint\n",
    "    list_endpoints_response = sm_client.list_endpoints(\n",
    "        SortBy=\"CreationTime\",\n",
    "        SortOrder=\"Descending\",\n",
    "        NameContains=endpoint_name,\n",
    "    )\n",
    "    print(f\"list_endpoints_response: {list_endpoints_response}\")\n",
    "\n",
    "    if len(list_endpoints_response[\"Endpoints\"]) > 0:\n",
    "        print(\"Updating Endpoint with new Endpoint Configuration\")\n",
    "        update_endpoint_response = sm_client.update_endpoint(\n",
    "            EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    "        )\n",
    "        print(f\"update_endpoint_response: {update_endpoint_response}\")\n",
    "    else:\n",
    "        print(\"Creating Endpoint\")\n",
    "        create_endpoint_response = sm_client.create_endpoint(\n",
    "            EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    "        )\n",
    "        print(f\"create_endpoint_response: {create_endpoint_response}\")\n",
    "\n",
    "    return {\"statusCode\": 200, \"body\": json.dumps(\"Endpoint Created Successfully\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "1397209d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ARN from existing role: deploy-model-lambda-role\n"
     ]
    }
   ],
   "source": [
    "from iam_helper import create_sagemaker_lambda_role\n",
    "\n",
    "lambda_role = create_sagemaker_lambda_role(\"deploy-model-lambda-role\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "25d717db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.lambda_step import LambdaStep\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "\n",
    "endpoint_config_name = \"linear-learner-air-quality-config\"\n",
    "endpoint_name = \"linear-learner-air-quality-endpoint-\" + current_time\n",
    "\n",
    "deploy_model_lambda_function_name = \"sagemaker-deploy-model-lambda-\" + current_time\n",
    "\n",
    "deploy_model_lambda_function = Lambda(\n",
    "    function_name=deploy_model_lambda_function_name,\n",
    "    execution_role_arn=lambda_role,\n",
    "    script=\"deploy_model_lambda.py\",\n",
    "    handler=\"deploy_model_lambda.lambda_handler\",\n",
    ")\n",
    "\n",
    "step_deploy_predictor = LambdaStep(\n",
    "    name=\"Deploy-Linear-Learner-Air-Quality-Endpoint\",\n",
    "    lambda_func=deploy_model_lambda_function,\n",
    "    inputs={\n",
    "        \"model_name\": step_create_model.properties.ModelName,\n",
    "        \"endpoint_config_name\": endpoint_config_name,\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \"endpoint_instance_type\": transformer_instance_type,\n",
    "        \"model_monitoring_s3_capture_upload_path\": s3_capture_upload_path,\n",
    "    },\n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff5e677",
   "metadata": {},
   "source": [
    "## Batch Transformer Step\n",
    "\n",
    "The model can be either deployed for real time inference or set up to be run on batches of data with a transform job. Creating a `Transformer` from a sagemaker model creates a transformer which can be used to perform batch inference.\n",
    "\n",
    "When creating the transformer, the output defaults to the sagemaker defualt bucket. It can be specified with `output_path` to save to a more desirable location. The other relevant parameters are `instance_count` and `instance_type`, which dictate the number and size of instance that will run the transform job, `max_concurrent_transforms`, which determines how many HTTP requests can be made to each transform container at a time, and `max_payload`, which determines how many megabytes can be sent to a transformer at once (max 4).\n",
    "\n",
    "The transformer can then be passed to the TransformStep, which enables the pipeline to create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "85b3c8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_count=transformer_instance_count,\n",
    "    instance_type=transformer_instance_type,\n",
    "    max_concurrent_transforms=concurrency,\n",
    "    max_payload=max_payload_in_mb,\n",
    "    output_path=output_data_path,\n",
    ")\n",
    "\n",
    "step_batch_transform = TransformStep(\n",
    "    name=\"Create-Linear-Learner-Air-Quality-Transformer\",\n",
    "    transformer=transformer,\n",
    "    inputs=\n",
    "        sagemaker.inputs.TransformInput(\n",
    "            data=step_preprocess_data.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri, # Use the same data from S3 as before\n",
    "            data_type='S3Prefix',\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "    \n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f130d50",
   "metadata": {},
   "source": [
    "## Pipeline Creation: Orchestrate all steps\n",
    "\n",
    "Now that all pipeline steps are created, a pipeline is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "f3c1bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create SKlearn processor object,\n",
    "# The object contains information about what instance type to use, the IAM role to use etc.\n",
    "# A managed processor comes with a preconfigured container, so only specifying version is required.\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# Create a Sagemaker Pipeline.\n",
    "# Each parameter for the pipeline must be set as a parameter explicitly when the pipeline is created.\n",
    "# Also pass in each of the steps created above.\n",
    "# Note that the order of execution is determined from each step's dependencies on other steps,\n",
    "# not on the order they are passed in below.\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        training_instance_type,\n",
    "        input_data,\n",
    "        training_epochs,\n",
    "        transformer_instance_type,\n",
    "        transformer_instance_count,\n",
    "        max_payload_in_mb,\n",
    "        output_data_path,\n",
    "        concurrency,\n",
    "        nox_data,\n",
    "        weather_data,\n",
    "    ],\n",
    "    steps=[step_preprocess_data, step_train_model, step_create_model, step_batch_transform, step_deploy_predictor],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ef131c",
   "metadata": {},
   "source": [
    "## Execute the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9988e98d",
   "metadata": {},
   "source": [
    "### List the execution steps to check out the status and artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "1528d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "# definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048d4168",
   "metadata": {},
   "source": [
    "### Submit pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "3e82eee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-west-1:648739860567:pipeline/linearlearnerairqualitypipeline',\n",
       " 'ResponseMetadata': {'RequestId': '2aaf00f9-26e5-4da6-876b-9451499dc9da',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '2aaf00f9-26e5-4da6-876b-9451499dc9da',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '99',\n",
       "   'date': 'Mon, 06 Jun 2022 15:25:35 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d94d90",
   "metadata": {},
   "source": [
    "### Execute pipeline using the default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "f141dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7cfb57",
   "metadata": {},
   "source": [
    "### Wait for pipeline to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a45984",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e04e34",
   "metadata": {},
   "source": [
    "## Visualize SageMaker Pipeline\n",
    "In SageMaker Studio, choose `SageMaker Components and registries` in the left pane and under `Pipelines`, click the pipeline that was created. Then all pipeline executions are shown, and the one just created should have a status of `Succeded`. Selecting that execution, the different pipeline steps can be tracked as they execute.\n",
    "\n",
    "![](images/pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e30b806",
   "metadata": {},
   "source": [
    "### Create a baselining job with training dataset\n",
    "Now that you have the training data ready in Amazon S3, start a job to suggest constraints. DefaultModelMonitor.suggest_baseline(..) starts a ProcessingJob using an Amazon SageMaker provided Model Monitor container to generate the constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c363ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_prefix = prefix + \"/baselining\"\n",
    "baseline_data_prefix = baseline_prefix + \"/data\"\n",
    "baseline_results_prefix = baseline_prefix + \"/results\"\n",
    "\n",
    "baseline_data_uri = \"s3://{}/{}\".format(bucket, baseline_data_prefix)\n",
    "baseline_results_uri = \"s3://{}/{}\".format(bucket, baseline_results_prefix)\n",
    "print(\"Baseline data uri: {}\".format(baseline_data_uri))\n",
    "print(\"Baseline results uri: {}\".format(baseline_results_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0f5bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data_uri + \"/training-dataset-with-header.csv\",\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=json.loads(pipeline.definition())['Steps'][0]['Arguments']['ProcessingOutputConfig']['Outputs'][2]['S3Output']['S3Uri']\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a40854",
   "metadata": {},
   "source": [
    "## Clean up (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b947198b",
   "metadata": {},
   "source": [
    "#### Delete the pipeline to keep the studio environment tidy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c26a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_sagemaker_pipeline(sm_client, pipeline_name):\n",
    "    try:\n",
    "        sm_client.delete_pipeline(\n",
    "            PipelineName=pipeline_name,\n",
    "        )\n",
    "        print(\"{} pipeline deleted\".format(pipeline_name))\n",
    "    except Exception as e:\n",
    "        print(\"{} \\n\".format(e))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc09ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_sagemaker_pipeline(client, pipeline_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de583586",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "\n",
    "### Irish Weather Data:\n",
    "Met Éireann retains Intellectual Property Rights and copyright over our data. If data are published in raw or processed format Met Éireann must be acknowledged as the source. Met Éireann does not accept any liability whatsoever for any error or omission in the data series, their availability, or for any loss or damage arising from their use. This work is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) License.\n",
    "\n",
    "### Irish Air Quality Data:\n",
    "EPA,\"EPA Ireland Archive of Nitrogen Oxides Monitoring Data\". Associated datasets and digitial information objects connected to this resource are available at: Secure Archive For Environmental Research Data (SAFER) managed by Environmental Protection Agency Ireland http://erc.epa.ie/safer/resource?id=216a8992-76e5-102b-aa08-55a7497570d3 (Last Accessed: 2018-06-30) (both require as their data usage license that they be credited)\n",
    "\n",
    "### Wind Rose Code\n",
    "The Air Quality Rose was adapted from Wind Rose code that was published on GitHub under a BSD-license:\n",
    "https://github.com/Geosyntec/cloudside\n",
    "\n",
    "The air quality rose is based on a function called \"rose\" is in the viz.py submodule:\n",
    "https://github.com/Geosyntec/cloudside/blob/master/cloudside/viz.py#L370\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
