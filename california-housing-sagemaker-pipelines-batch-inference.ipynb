{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Pipelines California Housing - Taking different steps based on model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates how to take different actions based on model performance in a SageMaker Pipeline.\n",
    "\n",
    "The steps in this pipeline include:\n",
    "* Preprocessing the California Housing dataset.\n",
    "* Train a TensorFlow2 Artificial Neural Network (ANN) Model.\n",
    "* Creating a Transform Job to run batch inference on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: sagemaker>=2.51.0 in /opt/conda/lib/python3.7/site-packages (2.83.0)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (0.2.8)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (1.21.5)\n",
      "Requirement already satisfied: attrs==20.3.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (20.3.0)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (0.1.5)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (3.20.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (20.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (1.5.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (1.0.1)\n",
      "Requirement already satisfied: boto3>=1.20.21 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.51.0) (1.21.34)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.20.21->sagemaker>=2.51.0) (0.5.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.20.21->sagemaker>=2.51.0) (1.0.0)\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.34 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.20.21->sagemaker>=2.51.0) (1.24.34)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=1.4.0->sagemaker>=2.51.0) (2.2.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker>=2.51.0) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker>=2.51.0) (2.4.6)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker>=2.51.0) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker>=2.51.0) (2.8.1)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.51.0) (0.70.12.2)\n",
      "Requirement already satisfied: dill>=0.3.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.51.0) (0.3.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.51.0) (0.3.0)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.51.0) (1.6.6.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.25.0,>=1.24.34->boto3>=1.20.21->sagemaker>=2.51.0) (1.26.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install \"sagemaker>=2.51.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "model_package_group_name = \"TF2-California-Housing\"  # Model name in model registry\n",
    "prefix = \"tf2-california-housing-pipelines\"\n",
    "pipeline_name = \"TF2CaliforniaHousingPipeline\"  # SageMaker Pipeline name\n",
    "current_time = time.strftime(\"%m-%d-%H-%M-%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download California Housing dataset and upload to Amazon S3\n",
    "\n",
    "We use the California housing dataset.\n",
    "\n",
    "More info on the dataset:\n",
    "\n",
    "This dataset was obtained from the `StatLib` repository. http://lib.stat.cmu.edu/datasets/\n",
    "\n",
    "The target variable is the median house value for California districts.\n",
    "\n",
    "This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), \"data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "raw_dir = os.path.join(os.getcwd(), \"data/raw\")\n",
    "os.makedirs(raw_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-sample-files/datasets/tabular/california_housing/cal_housing.tgz to ./cal_housing.tgz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://sagemaker-sample-files/datasets/tabular/california_housing/cal_housing.tgz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: CaliforniaHousing/cal_housing.data: Cannot change ownership to uid 10017, gid 166: Operation not permitted\n",
      "tar: CaliforniaHousing/cal_housing.domain: Cannot change ownership to uid 10017, gid 166: Operation not permitted\n",
      "tar: Exiting with failure status due to previous errors\n"
     ]
    }
   ],
   "source": [
    "!tar -zxf cal_housing.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"housingMedianAge\",\n",
    "    \"totalRooms\",\n",
    "    \"totalBedrooms\",\n",
    "    \"population\",\n",
    "    \"households\",\n",
    "    \"medianIncome\",\n",
    "    \"medianHouseValue\",\n",
    "]\n",
    "cal_housing_df = pd.read_csv(\"CaliforniaHousing/cal_housing.data\", names=columns, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housingMedianAge</th>\n",
       "      <th>totalRooms</th>\n",
       "      <th>totalBedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>medianIncome</th>\n",
       "      <th>medianHouseValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housingMedianAge  totalRooms  totalBedrooms  \\\n",
       "0    -122.23     37.88              41.0       880.0          129.0   \n",
       "1    -122.22     37.86              21.0      7099.0         1106.0   \n",
       "2    -122.24     37.85              52.0      1467.0          190.0   \n",
       "3    -122.25     37.85              52.0      1274.0          235.0   \n",
       "4    -122.25     37.85              52.0      1627.0          280.0   \n",
       "\n",
       "   population  households  medianIncome  medianHouseValue  \n",
       "0       322.0       126.0        8.3252          452600.0  \n",
       "1      2401.0      1138.0        8.3014          358500.0  \n",
       "2       496.0       177.0        7.2574          352100.0  \n",
       "3       558.0       219.0        5.6431          341300.0  \n",
       "4       565.0       259.0        3.8462          342200.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-1-648739860567/tf2-california-housing-pipelines/data/raw\n"
     ]
    }
   ],
   "source": [
    "X = cal_housing_df[\n",
    "    [\n",
    "        \"longitude\",\n",
    "        \"latitude\",\n",
    "        \"housingMedianAge\",\n",
    "        \"totalRooms\",\n",
    "        \"totalBedrooms\",\n",
    "        \"population\",\n",
    "        \"households\",\n",
    "        \"medianIncome\",\n",
    "    ]\n",
    "]\n",
    "Y = cal_housing_df[[\"medianHouseValue\"]] / 100000\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33)\n",
    "\n",
    "np.save(os.path.join(raw_dir, \"x_train.npy\"), x_train)\n",
    "np.save(os.path.join(raw_dir, \"x_test.npy\"), x_test)\n",
    "np.save(os.path.join(raw_dir, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(raw_dir, \"y_test.npy\"), y_test)\n",
    "rawdata_s3_prefix = \"{}/data/raw\".format(prefix)\n",
    "raw_s3 = sagemaker_session.upload_data(path=\"./data/raw/\", key_prefix=rawdata_s3_prefix)\n",
    "print(raw_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString, ParameterFloat\n",
    "\n",
    "# raw input data\n",
    "input_data = ParameterString(name=\"InputData\", default_value=raw_s3)\n",
    "\n",
    "# processing step parameters\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "# training step parameters\n",
    "training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.large\")\n",
    "training_epochs = ParameterString(name=\"TrainingEpochs\", default_value=\"1\")\n",
    "\n",
    "\n",
    "# Transformer step parameters\n",
    "transformer_instance_type = ParameterString(name=\"TransformInstanceType\", default_value=\"ml.m5.large\")\n",
    "transformer_instance_count = ParameterInteger(name=\"TransformInstanceCount\", default_value=2)\n",
    "max_payload_in_mb = ParameterInteger(name=\"MaxPayloadMB\", default_value=2)\n",
    "output_data_path = ParameterString(name=\"OutputDataS3Path\",default_value=\"s3://{}/ca-housing-batch-infer\".format(bucket))\n",
    "concurrency = ParameterInteger(name=\"MaxConcurrentRequests\",default_value=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Step \n",
    "\n",
    "The first step in the pipeline will preprocess the data to prepare it for training. We create a `SKLearnProcessor` object similar to the one above, but now parameterized, so we can separately track and change the job configuration as needed, for example to increase the instance type size and count to accommodate a growing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_files = glob.glob(\"{}/*.npy\".format(\"/opt/ml/processing/input\"))\n",
    "    print(\"\\nINPUT FILE LIST: \\n{}\\n\".format(input_files))\n",
    "    scaler = StandardScaler()\n",
    "    x_train = np.load(os.path.join(\"/opt/ml/processing/input\", \"x_train.npy\"))\n",
    "    scaler.fit(x_train)\n",
    "    for file in input_files:\n",
    "        raw = np.load(file)\n",
    "        # only transform feature columns\n",
    "        if \"y_\" not in file:\n",
    "            transformed = scaler.transform(raw)\n",
    "        if \"train\" in file:\n",
    "            if \"y_\" in file:\n",
    "                output_path = os.path.join(\"/opt/ml/processing/train\", \"y_train.npy\")\n",
    "                np.save(output_path, raw)\n",
    "                print(\"SAVED LABEL TRAINING DATA FILE\\n\")\n",
    "            else:\n",
    "                output_path = os.path.join(\"/opt/ml/processing/train\", \"x_train.npy\")\n",
    "                np.save(output_path, transformed)\n",
    "                print(\"SAVED TRANSFORMED TRAINING DATA FILE\\n\")\n",
    "        else:\n",
    "            if \"y_\" in file:\n",
    "                output_path = os.path.join(\"/opt/ml/processing/test\", \"y_test.npy\")\n",
    "#                 output_path_csv = os.path.join(\"/opt/ml/processing/testcsv\", \"y_test.csv\")\n",
    "#                 np.savetxt(output_path_csv, raw, delimiter=\",\")\n",
    "                np.save(output_path, raw)\n",
    "                print(\"SAVED LABEL TEST DATA FILE\\n\")\n",
    "            else:\n",
    "                output_path = os.path.join(\"/opt/ml/processing/test\", \"x_test.npy\")\n",
    "                output_path_csv = os.path.join(\"/opt/ml/processing/testcsv\", \"x_test.csv\")\n",
    "                np.savetxt(output_path_csv, transformed, delimiter=\",\")\n",
    "                np.save(output_path, transformed)\n",
    "                print(\"SAVED TRANSFORMED TEST DATA FILE\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "# Create SKlearn processor object,\n",
    "# The object contains information about what instance type to use, the IAM role to use etc.\n",
    "# A managed processor comes with a preconfigured container, so only specifying version is required.\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    role=role,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"tf2-california-housing-processing-job\",\n",
    ")\n",
    "\n",
    "# Use the sklearn_processor in a Sagemaker pipelines ProcessingStep\n",
    "step_preprocess_data = ProcessingStep(\n",
    "    name=\"Preprocess-California-Housing-Data\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "        ProcessingOutput(output_name=\"testcsv\",source=\"/opt/ml/processing/testcsv\"),\n",
    "    ],\n",
    "    code=\"preprocess.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model step\n",
    "In the second step, the train and validation output from the precious processing step are used to train a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "import time\n",
    "\n",
    "# Where to store the trained model\n",
    "model_path = f\"s3://{bucket}/{prefix}/model/\"\n",
    "\n",
    "hyperparameters = {\"epochs\": training_epochs}\n",
    "tensorflow_version = \"2.4.1\"\n",
    "python_version = \"py37\"\n",
    "\n",
    "tf2_estimator = TensorFlow(\n",
    "    source_dir=\"code\",\n",
    "    entry_point=\"train.py\",\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    framework_version=tensorflow_version,\n",
    "    role=role,\n",
    "    base_job_name=\"tf2-california-housing-train\",\n",
    "    output_path=model_path,\n",
    "    hyperparameters=hyperparameters,\n",
    "    py_version=python_version,\n",
    ")\n",
    "\n",
    "# Use the tf2_estimator in a Sagemaker pipelines ProcessingStep.\n",
    "# NOTE how the input to the training job directly references the output of the previous step.\n",
    "step_train_model = TrainingStep(\n",
    "    name=\"Train-California-Housing-Model\",\n",
    "    estimator=tf2_estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_preprocess_data.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data=step_preprocess_data.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "\n",
    "The model is created and the name of the model is provided to the Lambda function for deployment. The `CreateModelStep` dynamically assigns a name to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.step_collections import CreateModelStep\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "\n",
    "model = TensorFlowModel(\n",
    "    role=role,\n",
    "    model_data=step_train_model.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    framework_version=tensorflow_version,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"Create-California-Housing-Model\",\n",
    "    model=model,\n",
    "    inputs=sagemaker.inputs.CreateModelInput(instance_type=transformer_instance_type),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Transformer Step\n",
    "\n",
    "The model can be either deployed for real time inference or set up to be run on batches of data with a transform job. Creating a `Transformer` from a sagemaker model creates a transformer which can be used to perform batch inference.\n",
    "\n",
    "When creating the transformer, the output defaults to the sagemaker defualt bucket. It can be specified with `output_path` to save to a more desirable location. The other relevant parameters are `instance_count` and `instance_type`, which dictate the number and size of instance that will run the transform job, `max_concurrent_transforms`, which determines how many HTTP requests can be made to each transform container at a time, and `max_payload`, which determines how many megabytes can be sent to a transformer at once (max 4).\n",
    "\n",
    "The transformer can then be passed to the TransformStep, which enables the pipeline to create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_count=transformer_instance_count,\n",
    "    instance_type=transformer_instance_type,\n",
    "    max_concurrent_transforms=concurrency,\n",
    "    max_payload=max_payload_in_mb,\n",
    "    output_path=output_data_path,\n",
    ")\n",
    "\n",
    "step_batch_transform = TransformStep(\n",
    "    name=\"Create-Housing-Transformer\",\n",
    "    transformer=transformer,\n",
    "    inputs=\n",
    "        sagemaker.inputs.TransformInput(\n",
    "            data=step_preprocess_data.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"testcsv\"\n",
    "            ].S3Output.S3Uri, # Use the same data from S3 as before\n",
    "            data_type='S3Prefix',\n",
    "            content_type='text/csv'\n",
    "        )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Creation: Orchestrate all steps\n",
    "\n",
    "Now that all pipeline steps are created, a pipeline is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# Create a Sagemaker Pipeline.\n",
    "# Each parameter for the pipeline must be set as a parameter explicitly when the pipeline is created.\n",
    "# Also pass in each of the steps created above.\n",
    "# Note that the order of execution is determined from each step's dependencies on other steps,\n",
    "# not on the order they are passed in below.\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        training_instance_type,\n",
    "        input_data,\n",
    "        training_epochs,\n",
    "        transformer_instance_type,\n",
    "        transformer_instance_count,\n",
    "        max_payload_in_mb,\n",
    "        output_data_path,\n",
    "        concurrency,\n",
    "    ],\n",
    "    steps=[step_preprocess_data, step_train_model, step_create_model, step_batch_transform],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the execution steps to check out the status and artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.large'},\n",
       "  {'Name': 'TrainingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.large'},\n",
       "  {'Name': 'InputData',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://sagemaker-us-west-1-648739860567/tf2-california-housing-pipelines/data/raw'},\n",
       "  {'Name': 'TrainingEpochs', 'Type': 'String', 'DefaultValue': '1'},\n",
       "  {'Name': 'TransformInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.large'},\n",
       "  {'Name': 'TransformInstanceCount', 'Type': 'Integer', 'DefaultValue': 2},\n",
       "  {'Name': 'MaxPayloadMB', 'Type': 'Integer', 'DefaultValue': 2},\n",
       "  {'Name': 'OutputDataS3Path',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://sagemaker-us-west-1-648739860567/ca-housing-batch-infer'},\n",
       "  {'Name': 'MaxConcurrentRequests', 'Type': 'Integer', 'DefaultValue': 4}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'Preprocess-California-Housing-Data',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'},\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '746614075791.dkr.ecr.us-west-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/preprocess.py']},\n",
       "    'RoleArn': 'arn:aws:iam::648739860567:role/service-role/AmazonSageMaker-ExecutionRole-20220223T203754',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': 'Parameters.InputData'},\n",
       "       'LocalPath': '/opt/ml/processing/input',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-west-1-648739860567/Preprocess-California-Housing-Data-5d8c3d3d411f587618dac80fdcc4879e/input/code/preprocess.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-us-west-1-648739860567/Preprocess-California-Housing-Data-5d8c3d3d411f587618dac80fdcc4879e/output/train',\n",
       "        'LocalPath': '/opt/ml/processing/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-us-west-1-648739860567/Preprocess-California-Housing-Data-5d8c3d3d411f587618dac80fdcc4879e/output/test',\n",
       "        'LocalPath': '/opt/ml/processing/test',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'testcsv',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-us-west-1-648739860567/Preprocess-California-Housing-Data-5d8c3d3d411f587618dac80fdcc4879e/output/testcsv',\n",
       "        'LocalPath': '/opt/ml/processing/testcsv',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}},\n",
       "  {'Name': 'Train-California-Housing-Model',\n",
       "   'Type': 'Training',\n",
       "   'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "     'TrainingImage': '763104351884.dkr.ecr.us-west-1.amazonaws.com/tensorflow-training:2.4.1-cpu-py37',\n",
       "     'EnableSageMakerMetricsTimeSeries': True},\n",
       "    'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-west-1-648739860567/tf2-california-housing-pipelines/model/'},\n",
       "    'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "    'ResourceConfig': {'InstanceCount': 1,\n",
       "     'InstanceType': {'Get': 'Parameters.TrainingInstanceType'},\n",
       "     'VolumeSizeInGB': 30},\n",
       "    'RoleArn': 'arn:aws:iam::648739860567:role/service-role/AmazonSageMaker-ExecutionRole-20220223T203754',\n",
       "    'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.Preprocess-California-Housing-Data.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ContentType': 'text/csv',\n",
       "      'ChannelName': 'train'},\n",
       "     {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.Preprocess-California-Housing-Data.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ContentType': 'text/csv',\n",
       "      'ChannelName': 'test'}],\n",
       "    'HyperParameters': {'epochs': {'Get': 'Parameters.TrainingEpochs'},\n",
       "     'sagemaker_submit_directory': '\"s3://sagemaker-us-west-1-648739860567/tf2-california-housing-train-2022-06-03-17-57-24-100/source/sourcedir.tar.gz\"',\n",
       "     'sagemaker_program': '\"train.py\"',\n",
       "     'sagemaker_container_log_level': '20',\n",
       "     'sagemaker_region': '\"us-west-1\"',\n",
       "     'model_dir': '\"s3://sagemaker-us-west-1-648739860567/tf2-california-housing-pipelines/model/tf2-california-housing-train-2022-06-03-17-57-24-100/model\"'},\n",
       "    'DebugHookConfig': {'S3OutputPath': 's3://sagemaker-us-west-1-648739860567/tf2-california-housing-pipelines/model/',\n",
       "     'CollectionConfigurations': []},\n",
       "    'ProfilerRuleConfigurations': [{'RuleConfigurationName': 'ProfilerReport-1654279044',\n",
       "      'RuleEvaluatorImage': '685455198987.dkr.ecr.us-west-1.amazonaws.com/sagemaker-debugger-rules:latest',\n",
       "      'RuleParameters': {'rule_to_invoke': 'ProfilerReport'}}],\n",
       "    'ProfilerConfig': {'S3OutputPath': 's3://sagemaker-us-west-1-648739860567/tf2-california-housing-pipelines/model/'}}},\n",
       "  {'Name': 'Create-California-Housing-Model',\n",
       "   'Type': 'Model',\n",
       "   'Arguments': {'ExecutionRoleArn': 'arn:aws:iam::648739860567:role/service-role/AmazonSageMaker-ExecutionRole-20220223T203754',\n",
       "    'PrimaryContainer': {'Image': '763104351884.dkr.ecr.us-west-1.amazonaws.com/tensorflow-inference:2.4.1-cpu',\n",
       "     'Environment': {},\n",
       "     'ModelDataUrl': {'Get': 'Steps.Train-California-Housing-Model.ModelArtifacts.S3ModelArtifacts'}}}},\n",
       "  {'Name': 'Create-Housing-Transformer',\n",
       "   'Type': 'Transform',\n",
       "   'Arguments': {'ModelName': {'Get': 'Steps.Create-California-Housing-Model.ModelName'},\n",
       "    'TransformInput': {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "       'S3Uri': {'Get': \"Steps.Preprocess-California-Housing-Data.ProcessingOutputConfig.Outputs['testcsv'].S3Output.S3Uri\"}}},\n",
       "     'ContentType': 'text/csv'},\n",
       "    'TransformOutput': {'S3OutputPath': {'Get': 'Parameters.OutputDataS3Path'}},\n",
       "    'TransformResources': {'InstanceCount': {'Get': 'Parameters.TransformInstanceCount'},\n",
       "     'InstanceType': {'Get': 'Parameters.TransformInstanceType'}},\n",
       "    'MaxConcurrentTransforms': {'Get': 'Parameters.MaxConcurrentRequests'},\n",
       "    'MaxPayloadInMB': {'Get': 'Parameters.MaxPayloadMB'}}}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-west-1:648739860567:pipeline/tf2californiahousingpipeline',\n",
       " 'ResponseMetadata': {'RequestId': '504a15b1-a7e8-403b-a6ca-de6c03cbe1e7',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '504a15b1-a7e8-403b-a6ca-de6c03cbe1e7',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '96',\n",
       "   'date': 'Fri, 03 Jun 2022 17:57:24 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute pipeline using the default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for pipeline to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize SageMaker Pipeline\n",
    "In SageMaker Studio, choose `SageMaker Components and registries` in the left pane and under `Pipelines`, click the pipeline that was created. Then all pipeline executions are shown, and the one just created should have a status of `Succeded`. Selecting that execution, the different pipeline steps can be tracked as they execute.\n",
    "\n",
    "![](images/pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete the pipeline to keep the studio environment tidy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_sagemaker_pipeline(sm_client, pipeline_name):\n",
    "    try:\n",
    "        sm_client.delete_pipeline(\n",
    "            PipelineName=pipeline_name,\n",
    "        )\n",
    "        print(\"{} pipeline deleted\".format(pipeline_name))\n",
    "    except Exception as e:\n",
    "        print(\"{} \\n\".format(e))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_sagemaker_pipeline(client, pipeline_name)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-1:742091327244:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
